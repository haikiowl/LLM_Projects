{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0c2ec1-6c24-4dcd-bf8d-27b67bc2292f",
   "metadata": {},
   "source": [
    "# MULTI-MODEL CONVERSATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6124730-c3fc-4466-929a-3d83ee23179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the requirements \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown,display,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e77c4a-e7d0-46b2-b0a8-6c0f0021820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gemini model from google\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b5c3d-b04f-4066-ba06-603339aa6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the API keys for all the models\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"anthropic API key exists and begins {anthropic_api_key[:5]}\")\n",
    "\n",
    "else:\n",
    "    print(\"cluade API key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"google API key exists and begins {google_api_key[:5]}\")\n",
    "\n",
    "else:\n",
    "    print(\"google API key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"deepseek API key exists and begins {deepseek_api_key[:5]}\")\n",
    "\n",
    "else:\n",
    "    print(\"deepseek API key not set\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0700ea-7f5d-489a-b5d3-246291008864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the models\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da6063-5c6a-45d5-aa21-e3af888d7b72",
   "metadata": {},
   "source": [
    "## INTERVIEW BOT:\n",
    "\n",
    "#### a multi-model conversation simulation between GPT and Claude, where GPT functions as an interviewer and Claude acts as a candidate applying for a data scientist position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb62e1e-d528-4433-95d7-395525ed8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system = \"\"\"You are an AI interviewer responsible for evaluating candidates for a senior data science position. Your tasks include:\n",
    "\n",
    "1. Conducting the Interview:\n",
    "   - Pose 3 advanced-level questions covering topics such as machine learning algorithms, statistical modeling, big data technologies, data architecture, and AI integration.\n",
    "   - After each candidate response, acknowledge receipt without providing immediate feedback or assessment.\n",
    "\n",
    "2. Post-Interview Assessment:\n",
    "   - Once all questions have been answered, provide a comprehensive evaluation of the candidate's performance.\n",
    "   - Assign a score for each response based on technical accuracy, problem-solving skills, and communication clarity.\n",
    "   - Conclude with a decision regarding the candidate's suitability for the role.\n",
    "\n",
    "3. Conversation Termination:\n",
    "   - After delivering the final assessment, end the conversation without further interaction.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "1. Technical Accuracy: Assess the correctness and depth of the content in the candidate's responses.\n",
    "\n",
    "2. Problem-Solving Skills: Evaluate the candidate's ability to apply knowledge to practical scenarios and devise effective solutions.\n",
    "\n",
    "3. Communication Clarity: Consider how clearly and concisely the candidate conveys complex concepts.\n",
    "\n",
    "4. Overall Suitability:Based on the responses, determine if the candidate possesses the requisite expertise and competencies for the senior data science role.\n",
    "\n",
    "This prompt ensures that the AI interviewer maintains a structured approach by withholding assessments until the interview is complete and appropriately concluding the interaction thereafter.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e605c-fe9b-4afc-929b-ec979e222fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_system = \"\"\"You are a data scientist applying for a position at our company. Your objectives are:\n",
    "\n",
    "1. Greeting: Begin by greeting the interviewer before responding to the first question.\n",
    "\n",
    "2. Response Guidelines:\n",
    "   - Provide answers with utmost clarity and precision.\n",
    "   - Keep each response concise, limiting them to less than 200 words.\n",
    "\n",
    "Adhere to these guidelines throughout the interview process.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be27f3-749c-4b24-b39e-ea360a44e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi! Good Morning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941237de-f850-42bb-9b91-191c1b69f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4948630-9205-425c-8cf4-ccb07169cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2063e92-d18c-49ba-84ff-e0e223ef7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_message})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c08f50-4e1f-445e-8acc-dcaf14844a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665ff29-f0e2-4c54-bdbb-e7c6549e58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(8):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832a280-651d-433f-88cb-694c08fcc632",
   "metadata": {},
   "source": [
    "# MULTI MODEL INTERVIEW BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1937ba7-62a9-4360-b390-564f54283f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the API keys\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47672be8-b859-4d40-9cb5-1cd078008868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the models\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa6796-a5f8-4638-980a-9e1f1d223184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize client for google\n",
    "gemini_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3aa8f2-46a9-48e3-a524-a6824cc6b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model configurations\n",
    "gpt_model = \"gpt-4\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-1.5-pro\"\n",
    "\n",
    "\n",
    "\n",
    "def get_job_role():\n",
    "    \"\"\"Get job role input from the user\"\"\"\n",
    "    job_role = input(\"Enter the job role for the interview: \")\n",
    "    return job_role\n",
    "\n",
    "def create_system_prompts(job_role):\n",
    "    \"\"\"Create system prompts based on the job role\"\"\"\n",
    "    \n",
    "    gpt_prompt = f\"\"\"You are an interviewer conducting a technical job interview for a {job_role} position.\n",
    "Your task is to ask 3 precise, technical questions that are directly relevant to the {job_role} role.\n",
    "Ask one question at a time. Keep your questions concise and focused.\n",
    "After receiving answers from both candidates, briefly compare their responses and then ask the next question.\n",
    "After all questions are asked, evaluate both candidates and select one for the position.\n",
    "\n",
    "Example question format: \"Can you explain the concept of X as it relates to {job_role} work?\" \n",
    "\n",
    "Important instructions:\n",
    "1. Your questions should be technical and specific to the {job_role} role, not generic interview questions\n",
    "2. Provide ONLY the question itself - do not add any commentary about what you're doing or your role\n",
    "3. DO NOT mention that you are an AI or make any references to being an AI\n",
    "4. DO NOT start with phrases like \"Question:\" or \"As an interviewer:\" - just ask the question directly\n",
    "5. DO NOT say you don't have access to past content - you are the interviewer asking new questions \"\"\"\n",
    "\n",
    "    claude_prompt = f\"\"\"You are a candidate in a job interview for a {job_role} position.\\\n",
    "    Answer the interviewer's questions thoughtfully and professionally.\n",
    "Keep your answers concise and under 200 words.\n",
    "Focus on technical accuracy and practical experience.\n",
    "You do not know what the other candidate has answered.\"\"\"\n",
    "\n",
    "    gemini_prompt = f\"\"\"You are a candidate in a job interview for a {job_role} position. \n",
    "Answer the interviewer's questions thoughtfully and professionally.\n",
    "Keep your answers concise and under 200 words.\n",
    "Focus on technical accuracy and practical experience.\n",
    "You do not know what the other candidate has answered.\"\"\"\n",
    "\n",
    "    return gpt_prompt, claude_prompt, gemini_prompt\n",
    "\n",
    "# Function to have GPT ask a question or evaluate responses\n",
    "def call_gpt(conversation_history, gpt_system, is_final_evaluation=False, is_first_question=False):\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    \n",
    "    # Add all previous conversation to context\n",
    "    for entry in conversation_history:\n",
    "        if \"gpt_question\" in entry:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": entry[\"gpt_question\"]})\n",
    "        if \"claude_answer\" in entry:\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Claude's answer: {entry['claude_answer']}\"})\n",
    "        if \"gemini_answer\" in entry:\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Gemini's answer: {entry['gemini_answer']}\"})\n",
    "    \n",
    "    # If it's the final evaluation, add instruction to select a candidate\n",
    "    if is_final_evaluation:\n",
    "        messages.append({\"role\": \"user\", \"content\": \"Please evaluate both candidates based on all their answers and select one for the position. Provide your reasoning.\"})\n",
    "    elif is_first_question:\n",
    "      \n",
    "        messages.append({\"role\": \"user\", \"content\": \"You are the interviewer. Please ask your first technical question for this role. Make it precise and directly relevant to the role.\"})\n",
    "    else:\n",
    "     \n",
    "        messages.append({\"role\": \"user\", \"content\": \"Based on these responses, please ask your next precise technical question for this role.\"})\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling GPT: {e}\")\n",
    "        return \"Could not generate question. Please try again.\"\n",
    "\n",
    "# Function to get Claude's answer\n",
    "def call_claude(question, claude_system):\n",
    "    try:\n",
    "        message = claude.messages.create(\n",
    "            model=claude_model,\n",
    "            system=claude_system,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Interviewer's question: {question}. Remember to keep your answer under 200 words.\"}\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Claude: {e}\")\n",
    "        return \"Claude could not generate a response. Please try again.\"\n",
    "\n",
    "# Function to get Gemini's answer\n",
    "def call_gemini(question, gemini_system):\n",
    "    try:\n",
    "        response = gemini_client.chat.completions.create(\n",
    "            model=gemini_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": gemini_system},\n",
    "                {\"role\": \"user\", \"content\": f\"Interviewer's question: {question}. Remember to keep your answer under 200 words.\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini: {e}\")\n",
    "        return \"Gemini could not generate a response. Please try again.\"\n",
    "\n",
    "# Main interview function\n",
    "def conduct_interview(job_role, num_questions=5):\n",
    "    conversation_history = []\n",
    "    \n",
    "    # Create system prompts based on job role\n",
    "    gpt_system, claude_system, gemini_system = create_system_prompts(job_role)\n",
    "    \n",
    "    print(f\"\\n--- Starting Interview for {job_role} Position ---\\n\")\n",
    "    \n",
    "    # Initial question from GPT\n",
    "    initial_question = call_gpt(conversation_history, gpt_system, is_first_question=True)\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        # Get the current question from GPT\n",
    "        if i == 0:\n",
    "            current_question = initial_question\n",
    "        else:\n",
    "            current_question = call_gpt(conversation_history, gpt_system)\n",
    "        \n",
    "        # Add question to history\n",
    "        conversation_history.append({\"gpt_question\": current_question})\n",
    "        \n",
    "        # Get responses from both models\n",
    "        claude_answer = call_claude(current_question, claude_system)\n",
    "        gemini_answer = call_gemini(current_question, gemini_system)\n",
    "        \n",
    "        # Add responses to history\n",
    "        conversation_history[-1][\"claude_answer\"] = claude_answer\n",
    "        conversation_history[-1][\"gemini_answer\"] = gemini_answer\n",
    "        \n",
    "        print(f\"\\nQuestion {i+1}: {current_question}\")\n",
    "        print(f\"\\nClaude: {claude_answer}\")\n",
    "        print(f\"\\nGemini: {gemini_answer}\")\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_evaluation = call_gpt(conversation_history, gpt_system, is_final_evaluation=True)\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    print(final_evaluation)\n",
    "    \n",
    "    return conversation_history, final_evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6477c82-9754-415a-acc8-305ee18eee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the interview\n",
    "if __name__ == \"__main__\":\n",
    "    job_role = get_job_role()\n",
    "    print(f\"\\nStarting LLM Interview for {job_role} position with GPT as interviewer, Claude and Gemini as candidates...\")\n",
    "    conversation, evaluation = conduct_interview(job_role, num_questions=3)\n",
    "    print(\"\\nInterview complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e619cd6-b5fa-45b7-8574-3dc5f2c2f15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
